# ansible/playbooks/configure-nginx.yml
---
- name: Configure AI Model Endpoints on Multi-Cloud Infrastructure
  hosts: all
  become: yes
  gather_facts: yes
  
  vars:
    model_configs:
      AWS: 
        model_name: "BERT-Large-Uncased"
        model_version: "2.1.0"
        framework: "PyTorch"
        gpu_type: "Tesla-T4"
      Azure:
        model_name: "GPT-2-Medium"
        model_version: "1.5.0"
        framework: "TensorFlow"
        gpu_type: "Tesla-K80"
      GCP:
        model_name: "LLaMA-2-7B"
        model_version: "3.0.1"
        framework: "Transformers"
        gpu_type: "Tesla-T4"
  
  tasks:
    - name: Update package cache (Debian/Ubuntu)
      apt:
        update_cache: yes
      when: ansible_os_family == "Debian"
    
    - name: Update package cache (RedHat/Amazon)
      yum:
        update_cache: yes
      when: ansible_os_family == "RedHat"
    
    - name: Install nginx
      package:
        name: nginx
        state: present
    
    - name: Create AI model directory
      file:
        path: /var/www/ai-endpoint
        state: directory
        owner: www-data
        group: www-data
        mode: '0755'
      when: ansible_os_family == "Debian"
    
    - name: Create AI model directory (RedHat)
      file:
        path: /var/www/ai-endpoint
        state: directory
        owner: nginx
        group: nginx
        mode: '0755'
      when: ansible_os_family == "RedHat"
    
    - name: Deploy AI endpoint HTML page
      template:
        src: ../templates/index.html.j2
        dest: /var/www/ai-endpoint/index.html
        mode: '0644'
      vars:
        model_name: "{{ model_configs[cloud_provider].model_name }}"
        model_version: "{{ model_configs[cloud_provider].model_version }}"
        framework: "{{ model_configs[cloud_provider].framework }}"
        gpu_type: "{{ model_configs[cloud_provider].gpu_type }}"
    
    - name: Create mock API response
      copy:
        content: |
          {
            "status": "healthy",
            "model": "{{ model_configs[cloud_provider].model_name }}",
            "version": "{{ model_configs[cloud_provider].model_version }}",
            "cloud_provider": "{{ cloud_provider }}",
            "inference_time_ms": {{ 20 + (range(1, 50) | random) }},
            "confidence": 0.{{ range(85, 99) | random }},
            "tokens_processed": {{ range(100, 1000) | random }},
            "gpu_utilization": {{ range(60, 95) | random }},
            "timestamp": "{{ ansible_date_time.iso8601 }}"
          }
        dest: /var/www/ai-endpoint/predict.json
        mode: '0644'
    
    - name: Configure nginx site
      copy:
        content: |
          server {
              listen 80 default_server;
              listen [::]:80 default_server;
              server_name _;
              
              root /var/www/ai-endpoint;
              index index.html;
              
              # Main AI endpoint page
              location / {
                  try_files $uri $uri/ =404;
              }
              
              # Mock AI prediction endpoint
              location /v1/models/predict {
                  default_type application/json;
                  try_files /predict.json =404;
                  add_header X-Model-Name "{{ model_configs[cloud_provider].model_name }}";
                  add_header X-Cloud-Provider "{{ cloud_provider }}";
              }
              
              # Health check endpoint
              location /health {
                  default_type application/json;
                  return 200 '{"status":"healthy","provider":"{{ cloud_provider }}"}';
              }
              
              # Metrics endpoint (Prometheus-style)
              location /metrics {
                  default_type text/plain;
                  return 200 'ai_model_requests_total{provider="{{ cloud_provider }}"} {{ range(1000, 5000) | random }}
          ai_model_latency_seconds{provider="{{ cloud_provider }}"} 0.0{{ range(10, 99) | random }}
          ai_model_gpu_utilization{provider="{{ cloud_provider }}"} 0.{{ range(60, 95) | random }}';
              }
          }
        dest: /etc/nginx/sites-available/default
      notify: restart nginx
    
    - name: Ensure nginx is started and enabled
      systemd:
        name: nginx
        state: started
        enabled: yes
    
    - name: Create AI model info file (for verification)
      copy:
        content: |
          ===================================
          CloudGuard AI Platform - Model Deployment
          ===================================
          Cloud Provider: {{ cloud_provider }}
          Model: {{ model_configs[cloud_provider].model_name }}
          Version: {{ model_configs[cloud_provider].model_version }}
          Framework: {{ model_configs[cloud_provider].framework }}
          GPU Type: {{ model_configs[cloud_provider].gpu_type }} (Simulated)
          Endpoint: http://{{ ansible_default_ipv4.address }}/
          API Endpoint: http://{{ ansible_default_ipv4.address }}/v1/models/predict
          Health Check: http://{{ ansible_default_ipv4.address }}/health
          Metrics: http://{{ ansible_default_ipv4.address }}/metrics
          ===================================
        dest: /home/{{ ansible_user }}/deployment-info.txt
        mode: '0644'
    
    - name: Display deployment information
      debug:
        msg:
          - "âœ… AI Model Endpoint Deployed Successfully!"
          - "Provider: {{ cloud_provider }}"
          - "Model: {{ model_configs[cloud_provider].model_name }}"
          - "URL: http://{{ ansible_default_ipv4.address }}/"
          - "API: http://{{ ansible_default_ipv4.address }}/v1/models/predict"
  
  handlers:
    - name: restart nginx
      systemd:
        name: nginx
        state: restarted